{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvzb8KahyAtY"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "### EMOTION PREDICTION IN CONVERSATIONS:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkNDkMK9yq01"
   },
   "source": [
    "### LOADING DATA\n",
    "- DATASET : MELD : https://affective-meld.github.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhE5XmMQqJfR"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_sent.csv')\n",
    "cv = pd.read_csv('dev_sent.csv')\n",
    "test = pd.read_csv('test_sent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrSWw7k1yuUr"
   },
   "source": [
    "### EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "eSkliokBquNZ",
    "outputId": "6c44827e-050e-45b3-fff6-1ac55dc595fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Sentiment', 'Dialogue_ID',\n",
       "       'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ZEEkh1bprrWx",
    "outputId": "d025f389-6705-4205-ba54-d84f1b1432f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my companys tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You mustve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So lets talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Now youll be heading a whole division, so you...</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:41,126</td>\n",
       "      <td>00:16:44,337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>I see.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:48,800</td>\n",
       "      <td>00:16:51,886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>But therell be perhaps 30 people under you so...</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:48,800</td>\n",
       "      <td>00:16:54,514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Good to know.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:59,477</td>\n",
       "      <td>00:17:00,478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>We can go into detail</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:17:00,478</td>\n",
       "      <td>00:17:02,719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance          Speaker  \\\n",
       "0       1  also I was the point person on my companys tr...         Chandler   \n",
       "1       2                   You mustve had your hands full.  The Interviewer   \n",
       "2       3                            That I did. That I did.         Chandler   \n",
       "3       4      So lets talk a little bit about your duties.  The Interviewer   \n",
       "4       5                             My duties?  All right.         Chandler   \n",
       "5       6  Now youll be heading a whole division, so you...  The Interviewer   \n",
       "6       7                                             I see.         Chandler   \n",
       "7       8  But therell be perhaps 30 people under you so...  The Interviewer   \n",
       "8       9                                      Good to know.         Chandler   \n",
       "9      10                              We can go into detail  The Interviewer   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            0             0       8       21   \n",
       "1   neutral   neutral            0             1       8       21   \n",
       "2   neutral   neutral            0             2       8       21   \n",
       "3   neutral   neutral            0             3       8       21   \n",
       "4  surprise  positive            0             4       8       21   \n",
       "5   neutral   neutral            0             5       8       21   \n",
       "6   neutral   neutral            0             6       8       21   \n",
       "7   neutral   neutral            0             7       8       21   \n",
       "8   neutral   neutral            0             8       8       21   \n",
       "9   neutral   neutral            0             9       8       21   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:16:16,059  00:16:21,731  \n",
       "1  00:16:21,940  00:16:23,442  \n",
       "2  00:16:23,442  00:16:26,389  \n",
       "3  00:16:26,820  00:16:29,572  \n",
       "4  00:16:34,452  00:16:40,917  \n",
       "5  00:16:41,126  00:16:44,337  \n",
       "6  00:16:48,800  00:16:51,886  \n",
       "7  00:16:48,800  00:16:54,514  \n",
       "8  00:16:59,477  00:17:00,478  \n",
       "9  00:17:00,478  00:17:02,719  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9989, 11), (1109, 11), (2610, 11))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, cv.shape , test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9989 entries, 0 to 9988\n",
      "Data columns (total 11 columns):\n",
      "Sr No.          9989 non-null int64\n",
      "Utterance       9989 non-null object\n",
      "Speaker         9989 non-null object\n",
      "Emotion         9989 non-null object\n",
      "Sentiment       9989 non-null object\n",
      "Dialogue_ID     9989 non-null int64\n",
      "Utterance_ID    9989 non-null int64\n",
      "Season          9989 non-null int64\n",
      "Episode         9989 non-null int64\n",
      "StartTime       9989 non-null object\n",
      "EndTime         9989 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 858.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2610 entries, 0 to 2609\n",
      "Data columns (total 11 columns):\n",
      "Sr No.          2610 non-null int64\n",
      "Utterance       2610 non-null object\n",
      "Speaker         2610 non-null object\n",
      "Emotion         2610 non-null object\n",
      "Sentiment       2610 non-null object\n",
      "Dialogue_ID     2610 non-null int64\n",
      "Utterance_ID    2610 non-null int64\n",
      "Season          2610 non-null int64\n",
      "Episode         2610 non-null int64\n",
      "StartTime       2610 non-null object\n",
      "EndTime         2610 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 224.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1109 entries, 0 to 1108\n",
      "Data columns (total 11 columns):\n",
      "Sr No.          1109 non-null int64\n",
      "Utterance       1109 non-null object\n",
      "Speaker         1109 non-null object\n",
      "Emotion         1109 non-null object\n",
      "Sentiment       1109 non-null object\n",
      "Dialogue_ID     1109 non-null int64\n",
      "Utterance_ID    1109 non-null int64\n",
      "Season          1109 non-null int64\n",
      "Episode         1109 non-null int64\n",
      "StartTime       1109 non-null object\n",
      "EndTime         1109 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 95.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "rKcSVCMIr4g_",
    "outputId": "a5d6fc39-5c8d-45a0-a427-7373a768b840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     4710\n",
      "joy         1743\n",
      "surprise    1205\n",
      "anger       1109\n",
      "sadness      683\n",
      "disgust      271\n",
      "fear         268\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Imbalanced Data\n",
    "ax = sns.countplot(x=train['Emotion'], data=train)\n",
    "print(train['Emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "_NctoU2quOfz",
    "outputId": "7a6068a2-d0e1-470f-bfbf-80a94ca38018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     470\n",
      "joy         163\n",
      "anger       153\n",
      "surprise    150\n",
      "sadness     111\n",
      "fear         40\n",
      "disgust      22\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Imbalanced Data\n",
    "ax = sns.countplot(x=cv['Emotion'], data=cv)\n",
    "print(cv['Emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "Y5lbjooKuOjM",
    "outputId": "a55d1134-f15e-486a-fa15-f44e65c4f264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     1256\n",
      "joy          402\n",
      "anger        345\n",
      "surprise     281\n",
      "sadness      208\n",
      "disgust       68\n",
      "fear          50\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Imbalanced Data\n",
    "ax = sns.countplot(x=test['Emotion'], data=test)\n",
    "print(test['Emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:-\n",
    "- Their is no missing values present in our dataset.\n",
    "- the distribution of train , cv and test data points are same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "um7uQHVnvVg1"
   },
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69sLvs-lvDjb"
   },
   "outputs": [],
   "source": [
    "X_train , y_train = train[['Utterance']] , train[['Emotion']]\n",
    "X_cv , y_cv = cv[['Utterance']] , cv[['Emotion']]\n",
    "X_test , y_test = test[['Utterance']] , test[['Emotion']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "### Dataset Preprocessing training set\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "train_corpus = []\n",
    "for i in range(0, len(X_train)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', X_train['Utterance'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    train_corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "### Dataset Preprocessing cv set\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "cv_corpus = []\n",
    "for i in range(0, len(X_cv)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', X_cv['Utterance'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    cv_corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "### Dataset Preprocessing test set\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "test_corpus = []\n",
    "for i in range(0, len(X_test)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', X_test['Utterance'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    test_corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvhtwpVXy1DT"
   },
   "source": [
    "## Defining Multi Class LogLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3PWj4wgvkhO"
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEbiFXPFy6f6"
   },
   "source": [
    "### Define a scorer to be used in Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2hnrIy-2KQQ"
   },
   "outputs": [],
   "source": [
    "scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-3Gv2UTxdk4"
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRPTJO20vzar"
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.LabelEncoder()\n",
    "y_train_enc = enc.fit_transform(y_train.Emotion.values)\n",
    "y_cv_enc = enc.transform(y_cv.Emotion.values)\n",
    "y_test_enc = enc.transform(y_test.Emotion.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['clean_utterance'] = train_corpus\n",
    "X_train.drop('Utterance',axis=1,inplace=True)\n",
    "X_cv['clean_utterance'] = cv_corpus\n",
    "X_cv.drop('Utterance',axis=1,inplace=True)\n",
    "X_test['clean_utterance'] = test_corpus\n",
    "X_test.drop('Utterance',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vj6KJRzkzKMp"
   },
   "source": [
    "###  MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIFLFPAUznGR"
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89fw_nBvzjaz"
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(X_train['clean_utterance']) + list(X_cv['clean_utterance']) + list(X_test['clean_utterance']))\n",
    "X_train_tfv =  tfv.transform(X_train['clean_utterance']) \n",
    "X_valid_tfv = tfv.transform(X_cv['clean_utterance'])\n",
    "X_test_tfv = tfv.transform(X_test['clean_utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9989, 3179)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1109, 3179)\n"
     ]
    }
   ],
   "source": [
    "X_valid_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    7.1s finished\n",
      "Best score: -1.417605 using {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "\n",
    "\n",
    "# initialize Our first RandomForestRegressor model...\n",
    "regr2 = LogisticRegression()\n",
    "\n",
    "# declare parameters for hyperparameter tuning\n",
    "parameters = {'C':alpha} \n",
    "\n",
    "# Perform cross validation \n",
    "clf = GridSearchCV(regr2,\n",
    "                    param_grid = parameters,\n",
    "                    scoring=scorer,\n",
    "                    n_jobs = -1,\n",
    "                    verbose = 10, refit=True, cv=2)\n",
    "result = clf.fit(X_train_tfv, y_train_enc)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best score: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "aAyCwQ9i8M03",
    "outputId": "2ceb705e-8ca5-4783-e3ee-a6d293a829fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 1)\n",
    "lr.fit(X_train_tfv, y_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoJ9Wtgz9AaV"
   },
   "source": [
    "## Train,Test and CV loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.119 \n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict_proba(X_train_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_train_enc, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.325 \n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict_proba(X_valid_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_cv_enc, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.271 \n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict_proba(X_test_tfv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test_enc, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otBqIIeLeMFu"
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "a =list(X_train['clean_utterance']) + list(X_cv['clean_utterance']) + list(X_test['clean_utterance'])\n",
    "token.fit_on_texts(a)\n",
    "xtrain_seq = token.texts_to_sequences(X_train['clean_utterance'])\n",
    "xvalid_seq = token.texts_to_sequences(X_cv['clean_utterance'])\n",
    "xtest_seq = token.texts_to_sequences(X_test['clean_utterance'])\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNfK4oehhmWO"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0s5IoVT6gqwI",
    "outputId": "799200a9-8992-4545-d574-961e2971c40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9989 samples, validate on 1109 samples\n",
      "Epoch 1/200\n",
      "9989/9989 [==============================] - 7s 658us/step - loss: 1.9394 - val_loss: 1.9347\n",
      "Epoch 2/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.9256 - val_loss: 1.9239\n",
      "Epoch 3/200\n",
      "9989/9989 [==============================] - 6s 593us/step - loss: 1.9122 - val_loss: 1.9134\n",
      "Epoch 4/200\n",
      "9989/9989 [==============================] - 6s 573us/step - loss: 1.8992 - val_loss: 1.9032\n",
      "Epoch 5/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.8867 - val_loss: 1.8934\n",
      "Epoch 6/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.8744 - val_loss: 1.8839\n",
      "Epoch 7/200\n",
      "9989/9989 [==============================] - 6s 591us/step - loss: 1.8627 - val_loss: 1.8746\n",
      "Epoch 8/200\n",
      "9989/9989 [==============================] - 6s 589us/step - loss: 1.8512 - val_loss: 1.8656\n",
      "Epoch 9/200\n",
      "9989/9989 [==============================] - 6s 593us/step - loss: 1.8400 - val_loss: 1.8569\n",
      "Epoch 10/200\n",
      "9989/9989 [==============================] - 6s 582us/step - loss: 1.8292 - val_loss: 1.8485\n",
      "Epoch 11/200\n",
      "9989/9989 [==============================] - 6s 586us/step - loss: 1.8188 - val_loss: 1.8402\n",
      "Epoch 12/200\n",
      "9989/9989 [==============================] - 6s 586us/step - loss: 1.8085 - val_loss: 1.8323\n",
      "Epoch 13/200\n",
      "9989/9989 [==============================] - 6s 590us/step - loss: 1.7987 - val_loss: 1.8245\n",
      "Epoch 14/200\n",
      "9989/9989 [==============================] - 6s 590us/step - loss: 1.7890 - val_loss: 1.8169\n",
      "Epoch 15/200\n",
      "9989/9989 [==============================] - 6s 577us/step - loss: 1.7797 - val_loss: 1.8095\n",
      "Epoch 16/200\n",
      "9989/9989 [==============================] - 6s 582us/step - loss: 1.7707 - val_loss: 1.8024\n",
      "Epoch 17/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.7618 - val_loss: 1.7954\n",
      "Epoch 18/200\n",
      "9989/9989 [==============================] - 6s 588us/step - loss: 1.7533 - val_loss: 1.7886\n",
      "Epoch 19/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.7450 - val_loss: 1.7820\n",
      "Epoch 20/200\n",
      "9989/9989 [==============================] - 6s 592us/step - loss: 1.7370 - val_loss: 1.7758\n",
      "Epoch 21/200\n",
      "9989/9989 [==============================] - 6s 581us/step - loss: 1.7292 - val_loss: 1.7697\n",
      "Epoch 22/200\n",
      "9989/9989 [==============================] - 6s 588us/step - loss: 1.7216 - val_loss: 1.7640\n",
      "Epoch 23/200\n",
      "9989/9989 [==============================] - 6s 570us/step - loss: 1.7144 - val_loss: 1.7582\n",
      "Epoch 24/200\n",
      "9989/9989 [==============================] - 6s 574us/step - loss: 1.7073 - val_loss: 1.7528\n",
      "Epoch 25/200\n",
      "9989/9989 [==============================] - 6s 579us/step - loss: 1.7005 - val_loss: 1.7476\n",
      "Epoch 26/200\n",
      "9989/9989 [==============================] - 6s 577us/step - loss: 1.6939 - val_loss: 1.7426\n",
      "Epoch 27/200\n",
      "9989/9989 [==============================] - 6s 563us/step - loss: 1.6875 - val_loss: 1.7376\n",
      "Epoch 28/200\n",
      "9989/9989 [==============================] - 6s 603us/step - loss: 1.6813 - val_loss: 1.7330\n",
      "Epoch 29/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.6755 - val_loss: 1.7285\n",
      "Epoch 30/200\n",
      "9989/9989 [==============================] - 6s 611us/step - loss: 1.6698 - val_loss: 1.7242\n",
      "Epoch 31/200\n",
      "9989/9989 [==============================] - 6s 583us/step - loss: 1.6643 - val_loss: 1.7202\n",
      "Epoch 32/200\n",
      "9989/9989 [==============================] - 6s 578us/step - loss: 1.6590 - val_loss: 1.7162\n",
      "Epoch 33/200\n",
      "9989/9989 [==============================] - 6s 571us/step - loss: 1.6539 - val_loss: 1.7124\n",
      "Epoch 34/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.6490 - val_loss: 1.7088\n",
      "Epoch 35/200\n",
      "9989/9989 [==============================] - 6s 598us/step - loss: 1.6443 - val_loss: 1.7053\n",
      "Epoch 36/200\n",
      "9989/9989 [==============================] - 6s 579us/step - loss: 1.6398 - val_loss: 1.7021\n",
      "Epoch 37/200\n",
      "9989/9989 [==============================] - 6s 572us/step - loss: 1.6355 - val_loss: 1.6989\n",
      "Epoch 38/200\n",
      "9989/9989 [==============================] - 6s 579us/step - loss: 1.6313 - val_loss: 1.6959\n",
      "Epoch 39/200\n",
      "9989/9989 [==============================] - 6s 568us/step - loss: 1.6273 - val_loss: 1.6931\n",
      "Epoch 40/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.6235 - val_loss: 1.6903\n",
      "Epoch 41/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.6198 - val_loss: 1.6878\n",
      "Epoch 42/200\n",
      "9989/9989 [==============================] - 6s 595us/step - loss: 1.6163 - val_loss: 1.6853\n",
      "Epoch 43/200\n",
      "9989/9989 [==============================] - 6s 577us/step - loss: 1.6129 - val_loss: 1.6829\n",
      "Epoch 44/200\n",
      "9989/9989 [==============================] - 6s 572us/step - loss: 1.6097 - val_loss: 1.6808\n",
      "Epoch 45/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.6065 - val_loss: 1.6786\n",
      "Epoch 46/200\n",
      "9989/9989 [==============================] - 6s 589us/step - loss: 1.6036 - val_loss: 1.6765\n",
      "Epoch 47/200\n",
      "9989/9989 [==============================] - 6s 588us/step - loss: 1.6007 - val_loss: 1.6746\n",
      "Epoch 48/200\n",
      "9989/9989 [==============================] - 6s 604us/step - loss: 1.5980 - val_loss: 1.6728\n",
      "Epoch 49/200\n",
      "9989/9989 [==============================] - 6s 586us/step - loss: 1.5954 - val_loss: 1.6710\n",
      "Epoch 50/200\n",
      "9989/9989 [==============================] - 6s 572us/step - loss: 1.5929 - val_loss: 1.6694\n",
      "Epoch 51/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.5904 - val_loss: 1.6678\n",
      "Epoch 52/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5882 - val_loss: 1.6664\n",
      "Epoch 53/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5860 - val_loss: 1.6648\n",
      "Epoch 54/200\n",
      "9989/9989 [==============================] - 6s 568us/step - loss: 1.5839 - val_loss: 1.6636\n",
      "Epoch 55/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5819 - val_loss: 1.6624\n",
      "Epoch 56/200\n",
      "9989/9989 [==============================] - 6s 589us/step - loss: 1.5799 - val_loss: 1.6611\n",
      "Epoch 57/200\n",
      "9989/9989 [==============================] - 6s 588us/step - loss: 1.5781 - val_loss: 1.6599\n",
      "Epoch 58/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5763 - val_loss: 1.6588\n",
      "Epoch 59/200\n",
      "9989/9989 [==============================] - 6s 579us/step - loss: 1.5746 - val_loss: 1.6579\n",
      "Epoch 60/200\n",
      "9989/9989 [==============================] - 6s 582us/step - loss: 1.5730 - val_loss: 1.6568\n",
      "Epoch 61/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5715 - val_loss: 1.6558\n",
      "Epoch 62/200\n",
      "9989/9989 [==============================] - 6s 591us/step - loss: 1.5700 - val_loss: 1.6550\n",
      "Epoch 63/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5686 - val_loss: 1.6540\n",
      "Epoch 64/200\n",
      "9989/9989 [==============================] - 6s 590us/step - loss: 1.5673 - val_loss: 1.6532\n",
      "Epoch 65/200\n",
      "9989/9989 [==============================] - 6s 635us/step - loss: 1.5660 - val_loss: 1.6525\n",
      "Epoch 66/200\n",
      "9989/9989 [==============================] - 6s 633us/step - loss: 1.5647 - val_loss: 1.6517\n",
      "Epoch 67/200\n",
      "9989/9989 [==============================] - 6s 613us/step - loss: 1.5636 - val_loss: 1.6511\n",
      "Epoch 68/200\n",
      "9989/9989 [==============================] - 6s 608us/step - loss: 1.5624 - val_loss: 1.6504\n",
      "Epoch 69/200\n",
      "9989/9989 [==============================] - 6s 610us/step - loss: 1.5613 - val_loss: 1.6497\n",
      "Epoch 70/200\n",
      "9989/9989 [==============================] - 6s 631us/step - loss: 1.5603 - val_loss: 1.6492\n",
      "Epoch 71/200\n",
      "9989/9989 [==============================] - 6s 621us/step - loss: 1.5593 - val_loss: 1.6486\n",
      "Epoch 72/200\n",
      "9989/9989 [==============================] - 6s 641us/step - loss: 1.5584 - val_loss: 1.6480\n",
      "Epoch 73/200\n",
      "9989/9989 [==============================] - 6s 620us/step - loss: 1.5574 - val_loss: 1.6475\n",
      "Epoch 74/200\n",
      "9989/9989 [==============================] - 6s 624us/step - loss: 1.5566 - val_loss: 1.6470\n",
      "Epoch 75/200\n",
      "9989/9989 [==============================] - 6s 617us/step - loss: 1.5557 - val_loss: 1.6466\n",
      "Epoch 76/200\n",
      "9989/9989 [==============================] - 6s 629us/step - loss: 1.5550 - val_loss: 1.6461\n",
      "Epoch 77/200\n",
      "9989/9989 [==============================] - 6s 609us/step - loss: 1.5542 - val_loss: 1.6458\n",
      "Epoch 78/200\n",
      "9989/9989 [==============================] - 6s 616us/step - loss: 1.5534 - val_loss: 1.6452\n",
      "Epoch 79/200\n",
      "9989/9989 [==============================] - 6s 603us/step - loss: 1.5527 - val_loss: 1.6448\n",
      "Epoch 80/200\n",
      "9989/9989 [==============================] - 6s 634us/step - loss: 1.5521 - val_loss: 1.6445\n",
      "Epoch 81/200\n",
      "9989/9989 [==============================] - 6s 612us/step - loss: 1.5514 - val_loss: 1.6441\n",
      "Epoch 82/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5508 - val_loss: 1.6436\n",
      "Epoch 83/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5502 - val_loss: 1.6433\n",
      "Epoch 84/200\n",
      "9989/9989 [==============================] - 6s 581us/step - loss: 1.5496 - val_loss: 1.6430\n",
      "Epoch 85/200\n",
      "9989/9989 [==============================] - 6s 575us/step - loss: 1.5491 - val_loss: 1.6427\n",
      "Epoch 86/200\n",
      "9989/9989 [==============================] - 6s 617us/step - loss: 1.5486 - val_loss: 1.6424\n",
      "Epoch 87/200\n",
      "9989/9989 [==============================] - 6s 619us/step - loss: 1.5481 - val_loss: 1.6422\n",
      "Epoch 88/200\n",
      "9989/9989 [==============================] - 6s 612us/step - loss: 1.5476 - val_loss: 1.6420\n",
      "Epoch 89/200\n",
      "9989/9989 [==============================] - 6s 605us/step - loss: 1.5471 - val_loss: 1.6418\n",
      "Epoch 90/200\n",
      "9989/9989 [==============================] - 6s 613us/step - loss: 1.5467 - val_loss: 1.6414\n",
      "Epoch 91/200\n",
      "9989/9989 [==============================] - 6s 594us/step - loss: 1.5463 - val_loss: 1.6412\n",
      "Epoch 92/200\n",
      "9989/9989 [==============================] - 6s 597us/step - loss: 1.5459 - val_loss: 1.6410\n",
      "Epoch 93/200\n",
      "9989/9989 [==============================] - 6s 576us/step - loss: 1.5455 - val_loss: 1.6407\n",
      "Epoch 94/200\n",
      "9989/9989 [==============================] - 6s 591us/step - loss: 1.5451 - val_loss: 1.6406\n",
      "Epoch 95/200\n",
      "9989/9989 [==============================] - 6s 582us/step - loss: 1.5447 - val_loss: 1.6403\n",
      "Epoch 96/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5444 - val_loss: 1.6402\n",
      "Epoch 97/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5441 - val_loss: 1.6399\n",
      "Epoch 98/200\n",
      "9989/9989 [==============================] - 6s 582us/step - loss: 1.5437 - val_loss: 1.6398\n",
      "Epoch 99/200\n",
      "9989/9989 [==============================] - 6s 573us/step - loss: 1.5434 - val_loss: 1.6397\n",
      "Epoch 100/200\n",
      "9989/9989 [==============================] - 6s 583us/step - loss: 1.5431 - val_loss: 1.6395\n",
      "Epoch 101/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.5429 - val_loss: 1.6394\n",
      "Epoch 102/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.5426 - val_loss: 1.6391\n",
      "Epoch 103/200\n",
      "9989/9989 [==============================] - 6s 579us/step - loss: 1.5423 - val_loss: 1.6389\n",
      "Epoch 104/200\n",
      "9989/9989 [==============================] - 6s 602us/step - loss: 1.5421 - val_loss: 1.6387\n",
      "Epoch 105/200\n",
      "9989/9989 [==============================] - 6s 602us/step - loss: 1.5418 - val_loss: 1.6386\n",
      "Epoch 106/200\n",
      "9989/9989 [==============================] - 6s 603us/step - loss: 1.5416 - val_loss: 1.6385\n",
      "Epoch 107/200\n",
      "9989/9989 [==============================] - 6s 608us/step - loss: 1.5414 - val_loss: 1.6383\n",
      "Epoch 108/200\n",
      "9989/9989 [==============================] - 6s 593us/step - loss: 1.5412 - val_loss: 1.6383\n",
      "Epoch 109/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.5410 - val_loss: 1.6382\n",
      "Epoch 110/200\n",
      "9989/9989 [==============================] - 6s 604us/step - loss: 1.5408 - val_loss: 1.6380\n",
      "Epoch 111/200\n",
      "9989/9989 [==============================] - 6s 578us/step - loss: 1.5406 - val_loss: 1.6379\n",
      "Epoch 112/200\n",
      "9989/9989 [==============================] - 6s 586us/step - loss: 1.5404 - val_loss: 1.6378\n",
      "Epoch 113/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5403 - val_loss: 1.6377\n",
      "Epoch 114/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5401 - val_loss: 1.6375\n",
      "Epoch 115/200\n",
      "9989/9989 [==============================] - 6s 595us/step - loss: 1.5399 - val_loss: 1.6374\n",
      "Epoch 116/200\n",
      "9989/9989 [==============================] - 6s 604us/step - loss: 1.5398 - val_loss: 1.6373\n",
      "Epoch 117/200\n",
      "9989/9989 [==============================] - 6s 579us/step - loss: 1.5397 - val_loss: 1.6373\n",
      "Epoch 118/200\n",
      "9989/9989 [==============================] - 6s 600us/step - loss: 1.5395 - val_loss: 1.6373\n",
      "Epoch 119/200\n",
      "9989/9989 [==============================] - 6s 582us/step - loss: 1.5394 - val_loss: 1.6371\n",
      "Epoch 120/200\n",
      "9989/9989 [==============================] - 6s 577us/step - loss: 1.5393 - val_loss: 1.6371\n",
      "Epoch 121/200\n",
      "9989/9989 [==============================] - 6s 592us/step - loss: 1.5392 - val_loss: 1.6370\n",
      "Epoch 122/200\n",
      "9989/9989 [==============================] - 6s 592us/step - loss: 1.5390 - val_loss: 1.6368\n",
      "Epoch 123/200\n",
      "9989/9989 [==============================] - 6s 594us/step - loss: 1.5389 - val_loss: 1.6367\n",
      "Epoch 124/200\n",
      "9989/9989 [==============================] - 6s 592us/step - loss: 1.5388 - val_loss: 1.6367\n",
      "Epoch 125/200\n",
      "9989/9989 [==============================] - 6s 596us/step - loss: 1.5387 - val_loss: 1.6367\n",
      "Epoch 126/200\n",
      "9989/9989 [==============================] - 6s 583us/step - loss: 1.5386 - val_loss: 1.6366\n",
      "Epoch 127/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5385 - val_loss: 1.6365\n",
      "Epoch 128/200\n",
      "9989/9989 [==============================] - 6s 600us/step - loss: 1.5384 - val_loss: 1.6364\n",
      "Epoch 129/200\n",
      "9989/9989 [==============================] - 6s 601us/step - loss: 1.5383 - val_loss: 1.6365\n",
      "Epoch 130/200\n",
      "9989/9989 [==============================] - 6s 599us/step - loss: 1.5382 - val_loss: 1.6363\n",
      "Epoch 131/200\n",
      "9989/9989 [==============================] - 6s 591us/step - loss: 1.5382 - val_loss: 1.6364\n",
      "Epoch 132/200\n",
      "9989/9989 [==============================] - 6s 602us/step - loss: 1.5381 - val_loss: 1.6363\n",
      "Epoch 133/200\n",
      "9989/9989 [==============================] - 6s 595us/step - loss: 1.5380 - val_loss: 1.6361\n",
      "Epoch 134/200\n",
      "9989/9989 [==============================] - 6s 601us/step - loss: 1.5380 - val_loss: 1.6361\n",
      "Epoch 135/200\n",
      "9989/9989 [==============================] - 6s 599us/step - loss: 1.5379 - val_loss: 1.6361\n",
      "Epoch 136/200\n",
      "9989/9989 [==============================] - 6s 571us/step - loss: 1.5378 - val_loss: 1.6360\n",
      "Epoch 137/200\n",
      "9989/9989 [==============================] - 6s 595us/step - loss: 1.5378 - val_loss: 1.6360\n",
      "Epoch 138/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.5377 - val_loss: 1.6360\n",
      "Epoch 139/200\n",
      "9989/9989 [==============================] - 6s 595us/step - loss: 1.5377 - val_loss: 1.6359\n",
      "Epoch 140/200\n",
      "9989/9989 [==============================] - 6s 597us/step - loss: 1.5376 - val_loss: 1.6357\n",
      "Epoch 141/200\n",
      "9989/9989 [==============================] - 6s 584us/step - loss: 1.5376 - val_loss: 1.6357\n",
      "Epoch 142/200\n",
      "9989/9989 [==============================] - 6s 590us/step - loss: 1.5375 - val_loss: 1.6357\n",
      "Epoch 143/200\n",
      "9989/9989 [==============================] - 6s 597us/step - loss: 1.5375 - val_loss: 1.6358\n",
      "Epoch 144/200\n",
      "9989/9989 [==============================] - 6s 590us/step - loss: 1.5374 - val_loss: 1.6356\n",
      "Epoch 145/200\n",
      "9989/9989 [==============================] - 6s 581us/step - loss: 1.5374 - val_loss: 1.6357\n",
      "Epoch 146/200\n",
      "9989/9989 [==============================] - 6s 586us/step - loss: 1.5374 - val_loss: 1.6356\n",
      "Epoch 147/200\n",
      "9989/9989 [==============================] - 6s 585us/step - loss: 1.5373 - val_loss: 1.6355\n",
      "Epoch 148/200\n",
      "9989/9989 [==============================] - 6s 576us/step - loss: 1.5373 - val_loss: 1.6355\n",
      "Epoch 149/200\n",
      "9989/9989 [==============================] - 6s 578us/step - loss: 1.5373 - val_loss: 1.6355\n",
      "Epoch 150/200\n",
      "9989/9989 [==============================] - 6s 577us/step - loss: 1.5372 - val_loss: 1.6355\n",
      "Epoch 151/200\n",
      "9989/9989 [==============================] - 6s 599us/step - loss: 1.5372 - val_loss: 1.6354\n",
      "Epoch 152/200\n",
      "9989/9989 [==============================] - 6s 595us/step - loss: 1.5372 - val_loss: 1.6355\n",
      "Epoch 153/200\n",
      "9989/9989 [==============================] - 6s 588us/step - loss: 1.5371 - val_loss: 1.6354\n",
      "Epoch 154/200\n",
      "9989/9989 [==============================] - 6s 587us/step - loss: 1.5371 - val_loss: 1.6355\n"
     ]
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history=model.fit(xtrain_pad, y=y_train_enc_nn, batch_size=512, epochs=200, verbose=1, validation_data=(xvalid_pad, y_cv_enc_nn), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0COfXM9Sjiku"
   },
   "source": [
    "### Bi -LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yhBSQtbHiqVj",
    "outputId": "b890b452-dff3-4c47-974f-b043f2ed2e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9989 samples, validate on 1109 samples\n",
      "Epoch 1/200\n",
      "9989/9989 [==============================] - 13s 1ms/step - loss: 1.9397 - val_loss: 1.9351\n",
      "Epoch 2/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.9259 - val_loss: 1.9240\n",
      "Epoch 3/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.9122 - val_loss: 1.9135\n",
      "Epoch 4/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.8991 - val_loss: 1.9033\n",
      "Epoch 5/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8864 - val_loss: 1.8934\n",
      "Epoch 6/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8742 - val_loss: 1.8838\n",
      "Epoch 7/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.8623 - val_loss: 1.8745\n",
      "Epoch 8/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8508 - val_loss: 1.8655\n",
      "Epoch 9/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8396 - val_loss: 1.8567\n",
      "Epoch 10/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.8288 - val_loss: 1.8482\n",
      "Epoch 11/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8183 - val_loss: 1.8401\n",
      "Epoch 12/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8081 - val_loss: 1.8320\n",
      "Epoch 13/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7982 - val_loss: 1.8242\n",
      "Epoch 14/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7886 - val_loss: 1.8166\n",
      "Epoch 15/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7792 - val_loss: 1.8092\n",
      "Epoch 16/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7701 - val_loss: 1.8021\n",
      "Epoch 17/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7613 - val_loss: 1.7951\n",
      "Epoch 18/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7528 - val_loss: 1.7885\n",
      "Epoch 19/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.7445 - val_loss: 1.7819\n",
      "Epoch 20/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.7365 - val_loss: 1.7757\n",
      "Epoch 21/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.7287 - val_loss: 1.7696\n",
      "Epoch 22/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7212 - val_loss: 1.7637\n",
      "Epoch 23/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7139 - val_loss: 1.7582\n",
      "Epoch 24/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7069 - val_loss: 1.7527\n",
      "Epoch 25/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7001 - val_loss: 1.7475\n",
      "Epoch 26/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6936 - val_loss: 1.7425\n",
      "Epoch 27/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6873 - val_loss: 1.7376\n",
      "Epoch 28/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6811 - val_loss: 1.7329\n",
      "Epoch 29/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6752 - val_loss: 1.7285\n",
      "Epoch 30/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6695 - val_loss: 1.7243\n",
      "Epoch 31/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6641 - val_loss: 1.7201\n",
      "Epoch 32/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6588 - val_loss: 1.7161\n",
      "Epoch 33/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.6537 - val_loss: 1.7124\n",
      "Epoch 34/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6488 - val_loss: 1.7088\n",
      "Epoch 35/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6442 - val_loss: 1.7054\n",
      "Epoch 36/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6397 - val_loss: 1.7021\n",
      "Epoch 37/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6353 - val_loss: 1.6990\n",
      "Epoch 38/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6312 - val_loss: 1.6960\n",
      "Epoch 39/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6272 - val_loss: 1.6932\n",
      "Epoch 40/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6234 - val_loss: 1.6905\n",
      "Epoch 41/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6197 - val_loss: 1.6879\n",
      "Epoch 42/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.6162 - val_loss: 1.6854\n",
      "Epoch 43/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6128 - val_loss: 1.6831\n",
      "Epoch 44/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6096 - val_loss: 1.6808\n",
      "Epoch 45/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6065 - val_loss: 1.6786\n",
      "Epoch 46/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6035 - val_loss: 1.6767\n",
      "Epoch 47/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6007 - val_loss: 1.6747\n",
      "Epoch 48/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5979 - val_loss: 1.6729\n",
      "Epoch 49/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5954 - val_loss: 1.6712\n",
      "Epoch 50/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5928 - val_loss: 1.6695\n",
      "Epoch 51/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5904 - val_loss: 1.6679\n",
      "Epoch 52/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5882 - val_loss: 1.6663\n",
      "Epoch 53/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5860 - val_loss: 1.6650\n",
      "Epoch 54/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5839 - val_loss: 1.6637\n",
      "Epoch 55/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5819 - val_loss: 1.6623\n",
      "Epoch 56/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5799 - val_loss: 1.6611\n",
      "Epoch 57/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5781 - val_loss: 1.6599\n",
      "Epoch 58/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5763 - val_loss: 1.6588\n",
      "Epoch 59/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5746 - val_loss: 1.6578\n",
      "Epoch 60/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5730 - val_loss: 1.6567\n",
      "Epoch 61/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5715 - val_loss: 1.6558\n",
      "Epoch 62/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5700 - val_loss: 1.6549\n",
      "Epoch 63/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5686 - val_loss: 1.6541\n",
      "Epoch 64/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5673 - val_loss: 1.6533\n",
      "Epoch 65/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5660 - val_loss: 1.6525\n",
      "Epoch 66/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5647 - val_loss: 1.6519\n",
      "Epoch 67/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5636 - val_loss: 1.6511\n",
      "Epoch 68/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5624 - val_loss: 1.6505\n",
      "Epoch 69/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5614 - val_loss: 1.6499\n",
      "Epoch 70/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5603 - val_loss: 1.6492\n",
      "Epoch 71/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5593 - val_loss: 1.6486\n",
      "Epoch 72/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5584 - val_loss: 1.6481\n",
      "Epoch 73/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5575 - val_loss: 1.6475\n",
      "Epoch 74/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5566 - val_loss: 1.6471\n",
      "Epoch 75/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5558 - val_loss: 1.6466\n",
      "Epoch 76/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5549 - val_loss: 1.6460\n",
      "Epoch 77/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5542 - val_loss: 1.6457\n",
      "Epoch 78/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5535 - val_loss: 1.6453\n",
      "Epoch 79/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5527 - val_loss: 1.6449\n",
      "Epoch 80/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5521 - val_loss: 1.6445\n",
      "Epoch 81/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5514 - val_loss: 1.6441\n",
      "Epoch 82/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5508 - val_loss: 1.6437\n",
      "Epoch 83/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5502 - val_loss: 1.6434\n",
      "Epoch 84/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5496 - val_loss: 1.6431\n",
      "Epoch 85/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5491 - val_loss: 1.6428\n",
      "Epoch 86/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5486 - val_loss: 1.6425\n",
      "Epoch 87/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5481 - val_loss: 1.6422\n",
      "Epoch 88/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5476 - val_loss: 1.6420\n",
      "Epoch 89/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5471 - val_loss: 1.6417\n",
      "Epoch 90/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5467 - val_loss: 1.6415\n",
      "Epoch 91/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5462 - val_loss: 1.6412\n",
      "Epoch 92/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5458 - val_loss: 1.6410\n",
      "Epoch 93/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5454 - val_loss: 1.6408\n",
      "Epoch 94/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5451 - val_loss: 1.6406\n",
      "Epoch 95/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5447 - val_loss: 1.6403\n",
      "Epoch 96/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5444 - val_loss: 1.6402\n",
      "Epoch 97/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5441 - val_loss: 1.6398\n",
      "Epoch 98/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5437 - val_loss: 1.6396\n",
      "Epoch 99/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5434 - val_loss: 1.6395\n",
      "Epoch 100/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5431 - val_loss: 1.6392\n",
      "Epoch 101/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5428 - val_loss: 1.6391\n",
      "Epoch 102/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5426 - val_loss: 1.6390\n",
      "Epoch 103/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5423 - val_loss: 1.6388\n",
      "Epoch 104/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5421 - val_loss: 1.6387\n",
      "Epoch 105/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5418 - val_loss: 1.6386\n",
      "Epoch 106/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5416 - val_loss: 1.6385\n",
      "Epoch 107/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5414 - val_loss: 1.6383\n",
      "Epoch 108/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5412 - val_loss: 1.6384\n",
      "Epoch 109/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5410 - val_loss: 1.6381\n",
      "Epoch 110/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5408 - val_loss: 1.6380\n",
      "Epoch 111/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5406 - val_loss: 1.6379\n",
      "Epoch 112/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5404 - val_loss: 1.6378\n",
      "Epoch 113/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5403 - val_loss: 1.6377\n",
      "Epoch 114/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5401 - val_loss: 1.6376\n",
      "Epoch 115/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5399 - val_loss: 1.6374\n",
      "Epoch 116/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5398 - val_loss: 1.6374\n",
      "Epoch 117/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5397 - val_loss: 1.6373\n",
      "Epoch 118/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5395 - val_loss: 1.6372\n",
      "Epoch 119/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5394 - val_loss: 1.6371\n",
      "Epoch 120/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5392 - val_loss: 1.6370\n",
      "Epoch 121/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5391 - val_loss: 1.6370\n",
      "Epoch 122/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5390 - val_loss: 1.6369\n",
      "Epoch 123/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5389 - val_loss: 1.6368\n",
      "Epoch 124/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5388 - val_loss: 1.6366\n",
      "Epoch 125/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5387 - val_loss: 1.6366\n",
      "Epoch 126/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5386 - val_loss: 1.6366\n",
      "Epoch 127/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5385 - val_loss: 1.6366\n",
      "Epoch 128/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5384 - val_loss: 1.6365\n",
      "Epoch 129/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5383 - val_loss: 1.6364\n",
      "Epoch 130/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5382 - val_loss: 1.6363\n",
      "Epoch 131/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5382 - val_loss: 1.6363\n",
      "Epoch 132/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5381 - val_loss: 1.6363\n",
      "Epoch 133/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5380 - val_loss: 1.6361\n",
      "Epoch 134/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5379 - val_loss: 1.6362\n",
      "Epoch 135/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5379 - val_loss: 1.6361\n",
      "Epoch 136/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5378 - val_loss: 1.6360\n",
      "Epoch 137/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5378 - val_loss: 1.6360\n",
      "Epoch 138/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5377 - val_loss: 1.6360\n",
      "Epoch 139/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5377 - val_loss: 1.6359\n",
      "Epoch 140/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5376 - val_loss: 1.6358\n",
      "Epoch 141/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5376 - val_loss: 1.6359\n",
      "Epoch 142/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5375 - val_loss: 1.6358\n",
      "Epoch 143/200\n",
      "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5375 - val_loss: 1.6358\n",
      "Epoch 144/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5374 - val_loss: 1.6357\n",
      "Epoch 145/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5374 - val_loss: 1.6356\n",
      "Epoch 146/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5374 - val_loss: 1.6357\n",
      "Epoch 147/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5373 - val_loss: 1.6357\n",
      "Epoch 148/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5373 - val_loss: 1.6356\n",
      "Epoch 149/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5373 - val_loss: 1.6356\n",
      "Epoch 150/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5372 - val_loss: 1.6355\n",
      "Epoch 151/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5372 - val_loss: 1.6354\n",
      "Epoch 152/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5372 - val_loss: 1.6354\n",
      "Epoch 153/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6354\n",
      "Epoch 154/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6355\n",
      "Epoch 155/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6355\n",
      "Epoch 156/200\n",
      "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6355\n"
     ]
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history=model.fit(xtrain_pad, y=y_train_enc_nn, batch_size=512, epochs=200, verbose=1, validation_data=(xvalid_pad, y_cv_enc_nn), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7KmiMuK0GV-"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "joblib.dump(lr, 'lr2.pkl') \n",
    "joblib.dump(tfv, 'tfidf.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "def predictor(sentence):\n",
    "    lst = []\n",
    "    my_model = joblib.load('lr2.pkl') \n",
    "    tfv = joblib.load('tfidf.pkl')  \n",
    "\n",
    "    sent = preprocessor(sentence)\n",
    "\n",
    "    lst.append(sent)\n",
    "    sent_tfv = tfv.transform(lst)\n",
    "    ans = my_model.predict(sent_tfv) \n",
    "    mapped_ans = mapper(ans)\n",
    "    return mapped_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "### Dataset Preprocessing \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def preprocessor(sentence):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(val):\n",
    "    for i in val:\n",
    "    if i==0:\n",
    "        return 'Anger'\n",
    "    elif i==1:\n",
    "        return 'Disgust'\n",
    "    elif i==2:\n",
    "        return 'Fear'\n",
    "    elif i==3:\n",
    "        return 'Joy'\n",
    "    elif i==4:\n",
    "        return 'Neutral'\n",
    "    elif i==5:\n",
    "        return 'Sadness'\n",
    "    elif i==6:\n",
    "        return 'Surprise'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "val = predictor('oh! that is a good news')\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n"
     ]
    }
   ],
   "source": [
    "val = predictor('i do not like that thing')\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SaarthiAI.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
